{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# HW 5\n",
    "\n",
    "Muyuan Zhang\n",
    "\n",
    "u1430770\n",
    "\n",
    "07/09/2023"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Imports and setup. \n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math\n",
    "from sklearn import tree, svm, metrics\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.model_selection import train_test_split, cross_val_predict, cross_val_score, KFold\n",
    "from sklearn.datasets import load_digits\n",
    "from sklearn.preprocessing import scale\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = (10, 6)\n",
    "plt.style.use('ggplot')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: MNIST Handwritten Digits\n",
    "### Task 1.1: Classification with Support Vector Machines (SVM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "digits = load_digits()\n",
    "X = scale(digits.data)\n",
    "y = digits.target\n",
    "\n",
    "# split the data into a training and test set\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=1, test_size=0.8)\n",
    "\n",
    "# use SVM with an rbf kernel and the cost parameter C=5 \n",
    "# to build a classifier using the training dataset\n",
    "svm = SVC(kernel='rbf', C=5)\n",
    "svm.fit(X_train, y_train)\n",
    "\n",
    "# using the test dataset, evaluate the accuracy of the model\n",
    "accuracy = svm.score(X_test, y_test)\n",
    "print(\"The accuracy of the model is\", accuracy)\n",
    "\n",
    "# using the test dataset, compute the confusion matrix\n",
    "y_pred = svm.predict(X_test)\n",
    "confusion = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "print(\"\\nConfusion matrix\")\n",
    "print(confusion)\n",
    "\n",
    "# the most common mistake that the classifier makes\n",
    "np.fill_diagonal(confusion, 0)\n",
    "row, col = np.unravel_index(np.argmax(confusion), confusion.shape)\n",
    "\n",
    "print(\"\\nPredicted most common mistake\", col, \" Actual\", row)\n",
    "\n",
    "# display all of the misclassified digits as images\n",
    "misclassified_digits = np.where(y_test != y_pred)[0]\n",
    "plt.figure(figsize=(15, 15))\n",
    "num_rows = int(np.ceil(np.sqrt(len(misclassified_digits))))\n",
    "\n",
    "for i, index in enumerate(misclassified_digits):\n",
    "    plt.subplot(num_rows, num_rows, i + 1)\n",
    "    plt.imshow(np.reshape(X_test[index], (8, 8)), cmap='Greys')\n",
    "    plt.title(f\"Predicted: {y_pred[index]}\\nActual: {y_test[index]}\")\n",
    "    plt.axis('off')\n",
    "\n",
    "print(\"\\nAll the misclassified digits\")\n",
    "plt.tight_layout()  \n",
    "plt.show()\n",
    "\n",
    "# evaluate the accuracy of the SVM for different values of the parameter C\n",
    "C_values = np.concatenate((np.arange(0.5, 5.1, 0.1), np.arange(10, 51, 20)))\n",
    "accuracy_values = []\n",
    "\n",
    "for C in C_values:\n",
    "    svm = SVC(kernel='rbf', C=C)\n",
    "    scores = cross_val_score(svm, X, y, cv=5)\n",
    "    accuracy_values.append(np.mean(scores))\n",
    "\n",
    "plt.plot(C_values, accuracy_values, marker='o')\n",
    "plt.xlabel('C')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.title('Accuracy of the SVM for different C values')\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "print(\"The best value of C is\", C_values[np.argmax(accuracy_values)])\n",
    "\n",
    "# train and test the algorithm on the raw data\n",
    "X_train_raw, X_test_raw, y_train_raw, y_test_raw = train_test_split(digits.data, digits.target, random_state=1, test_size=0.8)\n",
    "svm_raw = SVC(kernel='rbf', C=5)\n",
    "svm_raw.fit(X_train_raw, y_train_raw)\n",
    "accuracy_raw = svm_raw.score(X_test_raw, y_test_raw)\n",
    "print(\"\\nThe accuracy of the raw data is\", accuracy_raw)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Interpretation of the results:\n",
    "\n",
    "The accuracy of the raw data is higher than that of the scaled data, because placing the data on a new scale means that unimportant or noise features cloud the signal.\n",
    "\n",
    "### Task 1.2: Prediction with K-nearest Neighbors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# split the data into a training and test set\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=1, test_size=0.8)\n",
    "\n",
    "# use k-NN to build a classifier using the training dataset\n",
    "knn = KNeighborsClassifier(n_neighbors=10)\n",
    "knn.fit(X_train, y_train)\n",
    "\n",
    "# using the test dataset, evaluate the accuracy of the model\n",
    "accuracy = knn.score(X_test, y_test)\n",
    "print(\"The accuracy of the k-NN model is\", accuracy)\n",
    "\n",
    "# using the test dataset, compute the confusion matrix\n",
    "y_pred = knn.predict(X_test)\n",
    "confusion = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "print(\"\\nConfusion matrix\")\n",
    "print(confusion)\n",
    "\n",
    "# the most common mistake that the classifier makes\n",
    "np.fill_diagonal(confusion, 0)\n",
    "row, col = np.unravel_index(np.argmax(confusion), confusion.shape)\n",
    "\n",
    "print(\"\\nPredicted most common mistake\", col, \" Actual\", row)\n",
    "\n",
    "# display all of the misclassified digits as images\n",
    "plt.style.use('ggplot')\n",
    "misclassified_digits = np.where(y_test != y_pred)[0]\n",
    "num_rows = math.ceil(len(misclassified_digits) / 5)\n",
    "num_cols = min(len(misclassified_digits), 5)\n",
    "plt.figure(figsize=(10, 2 * num_rows + 2))\n",
    "\n",
    "for i, index in enumerate(misclassified_digits):\n",
    "    predicted_digit = y_pred[index]\n",
    "    actual_digit = y_test[index]\n",
    "    plt.subplot(num_rows, num_cols, i + 1)\n",
    "    plt.imshow(np.reshape(X_test[index], (8, 8)), cmap='Greys', interpolation='nearest')\n",
    "    plt.title(f\"Predicted {predicted_digit}\\nActual {actual_digit}\")\n",
    "    plt.axis('off')\n",
    "\n",
    "print(\"\\nAll the misclassified digits\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# evaluate the accuracy of the k-NN for different values of the parameter k\n",
    "k_values = np.arange(1, 10)\n",
    "accuracy_values = []\n",
    "\n",
    "for k in k_values:\n",
    "    knn = KNeighborsClassifier(n_neighbors=k)\n",
    "    scores = cross_val_score(knn, X, y, cv=5)\n",
    "    accuracy_values.append(np.mean(scores))\n",
    "\n",
    "plt.plot(k_values, accuracy_values, marker='o')\n",
    "plt.xlabel('k')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.title('Accuracy of the k-NN for different k values')\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "print(\"The best value of k is\", k_values[np.argmax(accuracy_values)])\n",
    "\n",
    "X_train_raw, X_test_raw, y_train_raw, y_test_raw = train_test_split(\n",
    "    digits.data, digits.target, random_state=1, test_size=0.8)\n",
    "\n",
    "# train and test the algorithm on the raw data\n",
    "knn_raw = KNeighborsClassifier(n_neighbors=10)\n",
    "knn_raw.fit(X_train_raw, y_train_raw)\n",
    "accuracy_raw = knn_raw.score(X_test_raw, y_test_raw)\n",
    "print(\"\\nThe accuracy of the raw data is\", accuracy_raw)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Interpretation of the results:\n",
    "\n",
    "Again, the accuracy of the raw data is higher than that of the scaled data, but accuracy is not the best metric.\n",
    "\n",
    "## Part 2: Popularity of online news\n",
    "\n",
    "### Task 2.1: Import the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the dataset\n",
    "df = pd.read_csv('OnlineNewsPopularity/OnlineNewsPopularity.csv')\n",
    "df.columns = df.columns.str.strip()\n",
    "df = df.drop(['url', 'timedelta'], axis=1)\n",
    "X = df.drop('shares', axis=1).values\n",
    "# export the number of shares as a separate numpy array\n",
    "shares = df['shares'].values\n",
    "# create a binary numpy array which indicates whether or not each article is popular\n",
    "y = np.where(shares > np.median(shares), 1, 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 2.2: Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check to see if the values are reasonable\n",
    "print(df['shares'].describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 2.3: Classification Using k-NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# split the data into a training and test set\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "k_values = list(range(60, 81))\n",
    "accuracy_values = []\n",
    "\n",
    "# use cross validation to choose the best value of k\n",
    "for k in k_values:\n",
    "    knn = KNeighborsClassifier(n_neighbors=k)\n",
    "    scores = cross_val_score(knn, X_train_scaled, y_train, cv=5)\n",
    "    accuracy_values.append(scores.mean())\n",
    "\n",
    "print(\"The best k value is\", k_values[accuracy_values.index(max(accuracy_values))])\n",
    "print(\"The best accuracy is\", max(accuracy_values))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The best value of k is 73.\n",
    "\n",
    "The best accuracy I can obtain on the test data is 0.64.\n",
    "\n",
    "### Task 2.4 Classification using SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "fraction = 5000\n",
    "X_fraction = X[:fraction]\n",
    "y_fraction = y[:fraction]\n",
    "\n",
    "# split the data into a training and test set\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_fraction, y_fraction, test_size=0.2, random_state=42)\n",
    "C_values = list(range(48500, 49501, 100))\n",
    "accuracy_values = []\n",
    "\n",
    "# experiment with different Cs\n",
    "for C in C_values:\n",
    "    svm = SVC(C=C)\n",
    "    svm.fit(X_train, y_train)\n",
    "    y_pred = svm.predict(X_test)\n",
    "    accuracy_values.append(accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_C_values = [C_values[i]\n",
    "                    for i, acc in enumerate(accuracy_values) \n",
    "                    if acc == max(accuracy_values)]\n",
    "\n",
    "print(\"The best C values are\", best_C_values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 2.5 Classification using decision trees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "max_depth_values = list(range(5, 10))\n",
    "min_samples_split_values = list(range(15, 21))\n",
    "accuracy_values = []\n",
    "\n",
    "for max_depth in max_depth_values:\n",
    "    accuracy_row = []\n",
    "\n",
    "    for min_samples_split in min_samples_split_values:\n",
    "        tree = DecisionTreeClassifier(max_depth=max_depth, min_samples_split=min_samples_split)\n",
    "        scores = cross_val_score(tree, X, y, cv=5)\n",
    "        accuracy_row.append(scores.mean())\n",
    "    accuracy_values.append(accuracy_row)\n",
    "\n",
    "best_indices = np.unravel_index(np.argmax(accuracy_values), np.array(accuracy_values).shape)\n",
    "best_max_depth = max_depth_values[best_indices[0]]\n",
    "best_min_samples_split = min_samples_split_values[best_indices[1]]\n",
    "\n",
    "print(\"The best value of max tree depth is\", best_max_depth)\n",
    "print(\"The best value of minimum samples split is\", best_min_samples_split)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 2.6 Describe your findings\n",
    "\n",
    "Q: Which method (k-NN, SVM, Decision Tree) worked best?\n",
    "\n",
    "A: k-NN works best in accuracy.\n",
    "\n",
    "Q: How did different parameters influence the accuracy?\n",
    "\n",
    "A: \n",
    "\n",
    "* For k-NN, a small K leads to unstable decision boundaries, and a greater K value is better for classification as it leads to smoothening the decision boundaries but makes it computationally expensive. A simple approach to select k is to set k = n^(1/2).\n",
    "\n",
    "* For SVM, larger Cs allows for more complex decision boundaries, which can lead to overfitting, while smaller Cs may result in underfitting.\n",
    "\n",
    "* For Decision tree, the larger max_depth is, the more splits it has, and it captures more information about the data. Increasing min_samples_split value may cause underfitting.\n",
    "\n",
    "Q: Which model is easiest to interpret?\n",
    "\n",
    "A: Decision tree (easy to understand, can be visualized).\n",
    "\n",
    "Q: How would you interpret your results?\n",
    "\n",
    "A: The best value of each parameter in each model can be decided by iteration. In the cases above, the accuracy of the raw data is higher than that of the scaled data, but accuracy is not the best metric."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
